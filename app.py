"""
ğŸ¤– ØªØ·Ø¨ÙŠÙ‚ Streamlit Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… GLM API
Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ù† Zhipu AI
"""

import streamlit as st
from openai import OpenAI
import time

# ==================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ====================
st.set_page_config(
    page_title="GLM Chat - Ù…Ø­Ø§Ø¯Ø«Ø© Ø°ÙƒÙŠØ©",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================== Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø© ====================
GLM_MODELS = {
    "glm-4-plus": {
        "name": "GLM-4 Plus â­",
        "description": "Ø§Ù„Ø£Ø­Ø¯Ø« ÙˆØ§Ù„Ø£Ù‚ÙˆÙ‰ - Ø£Ø¯Ø§Ø¡ Ù…ØªÙÙˆÙ‚",
        "max_tokens": 128000,
        "recommended": True
    },
    "glm-4": {
        "name": "GLM-4",
        "description": "Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª",
        "max_tokens": 128000,
        "recommended": False
    },
    "glm-4-air": {
        "name": "GLM-4 Air ğŸš€",
        "description": "Ø³Ø±ÙŠØ¹ ÙˆÙØ¹Ø§Ù„ Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ÙŠÙˆÙ…ÙŠØ©",
        "max_tokens": 128000,
        "recommended": False
    },
    "glm-4-flash": {
        "name": "GLM-4 Flash âš¡",
        "description": "Ø§Ù„Ø£Ø³Ø±Ø¹ - Ù…Ø«Ø§Ù„ÙŠ Ù„Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ø±ÙŠØ¹Ø©",
        "max_tokens": 128000,
        "recommended": False
    },
    "glm-4-long": {
        "name": "GLM-4 Long ğŸ“š",
        "description": "Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚",
        "max_tokens": 1024000,
        "recommended": False
    },
    "glm-3-turbo": {
        "name": "GLM-3 Turbo",
        "description": "Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ø³Ø§Ø¨Ù‚ - Ø§Ù‚ØªØµØ§Ø¯ÙŠ",
        "max_tokens": 32000,
        "recommended": False
    }
}

# ==================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ ====================
with st.sidebar:
    st.title("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    
    # API Key
    api_key = st.text_input(
        "ğŸ”‘ API Key",
        value="f238665f81e44fad90c96cee0220b018.UnH1zIyvieg0zAnj",
        type="password",
        help="Ø£Ø¯Ø®Ù„ API Key Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù…Ù† open.bigmodel.cn"
    )
    
    st.divider()
    
    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    st.subheader("ğŸ§  Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§ Ø£ÙˆÙ„Ø§Ù‹)
    sorted_models = sorted(GLM_MODELS.items(), key=lambda x: not x[1]["recommended"])
    
    model_options = [f"{v['name']}" for k, v in sorted_models]
    model_keys = [k for k, v in sorted_models]
    
    selected_model_index = st.selectbox(
        "Ø§Ø®ØªØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:",
        range(len(model_options)),
        format_func=lambda i: model_options[i]
    )
    selected_model = model_keys[selected_model_index]
    model_info = GLM_MODELS[selected_model]
    
    st.caption(f"ğŸ“ {model_info['description']}")
    st.caption(f"ğŸ“Š Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰: {model_info['max_tokens']:,} tokens")
    
    st.divider()
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
    st.subheader("ğŸ›ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©")
    
    with st.expander("ğŸ”§ ØªØ®ØµÙŠØµ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª", expanded=False):
        temperature = st.slider(
            "ğŸŒ¡ï¸ Temperature",
            min_value=0.0,
            max_value=2.0,
            value=0.7,
            step=0.1,
            help="Ù‚ÙŠÙ… Ø£Ø¹Ù„Ù‰ = Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£ÙƒØ«Ø± Ø¥Ø¨Ø¯Ø§Ø¹Ø§Ù‹"
        )
        
        top_p = st.slider(
            "ğŸ¯ Top P",
            min_value=0.0,
            max_value=1.0,
            value=0.9,
            step=0.05,
            help="ØªÙ†ÙˆÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª"
        )
        
        max_tokens = st.slider(
            "ğŸ“ Max Tokens",
            min_value=100,
            max_value=min(4096, model_info["max_tokens"]),
            value=2048,
            step=100,
            help="Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„Ø±Ø¯"
        )
        
        stream_response = st.checkbox(
            "ğŸŒŠ Stream Mode",
            value=True,
            help="Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø¯ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹"
        )
    
    st.divider()
    
    # System Prompt
    st.subheader("ğŸ’¬ System Prompt")
    system_prompt = st.text_area(
        "ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:",
        value="Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ÙÙŠØ¯. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ù…Ù†Ùƒ ØºÙŠØ± Ø°Ù„Ùƒ.",
        height=100
    )
    
    st.divider()
    
    # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­ÙƒÙ…
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    with col2:
        if st.button("ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ†", use_container_width=True):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
    st.divider()
    st.markdown("""
    ### ğŸ“– Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
    
    **GLM** Ù‡ÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ù† **Zhipu AI**
    
    ğŸ”— [open.bigmodel.cn](https://open.bigmodel.cn)
    
    ---
    *ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨ÙˆØ§Ø³Ø·Ø© GLM API*
    """)

# ==================== Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ====================

def get_client(api_key: str) -> OpenAI:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù…ÙŠÙ„ OpenAI Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ GLM"""
    return OpenAI(
        api_key=api_key,
        base_url="https://open.bigmodel.cn/api/paas/v4/"
    )

def stream_chat(client: OpenAI, messages: list, model: str, **kwargs):
    """Ø¨Ø« Ø§Ù„Ø±Ø¯ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹"""
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=kwargs.get("temperature", 0.7),
        top_p=kwargs.get("top_p", 0.9),
        max_tokens=kwargs.get("max_tokens", 2048),
        stream=True
    )
    return response

def normal_chat(client: OpenAI, messages: list, model: str, **kwargs):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ ÙƒØ§Ù…Ù„Ø§Ù‹"""
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=kwargs.get("temperature", 0.7),
        top_p=kwargs.get("top_p", 0.9),
        max_tokens=kwargs.get("max_tokens", 2048),
        stream=False
    )
    return response

# ==================== ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ====================
if "messages" not in st.session_state:
    st.session_state.messages = []

# ==================== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ====================
st.title("ğŸ¤– GLM Chat - Ù…Ø­Ø§Ø¯Ø«Ø© Ø°ÙƒÙŠØ©")
st.caption(f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ: **{model_info['name']}** | {model_info['description']}")

# Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Ø­Ù‚Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
if prompt := st.chat_input("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§..."):
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† API Key
    if not api_key:
        st.error("âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ API Key")
    else:
        # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ù„Ù„Ø¥Ø±Ø³Ø§Ù„
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(st.session_state.messages)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯
        with st.chat_message("assistant"):
            try:
                client = get_client(api_key)
                
                if stream_response:
                    # ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø«
                    message_placeholder = st.empty()
                    full_response = ""
                    
                    response = stream_chat(
                        client, messages, selected_model,
                        temperature=temperature,
                        top_p=top_p,
                        max_tokens=max_tokens
                    )
                    
                    for chunk in response:
                        if chunk.choices[0].delta.content:
                            full_response += chunk.choices[0].delta.content
                            message_placeholder.markdown(full_response + "â–Œ")
                    
                    message_placeholder.markdown(full_response)
                else:
                    # Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¹Ø§Ø¯ÙŠ
                    with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙÙƒÙŠØ±..."):
                        response = normal_chat(
                            client, messages, selected_model,
                            temperature=temperature,
                            top_p=top_p,
                            max_tokens=max_tokens
                        )
                    
                    full_response = response.choices[0].message.content
                    st.markdown(full_response)
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø¯ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                
            except Exception as e:
                st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
                if "401" in str(e):
                    st.warning("âš ï¸ ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© API Key")
                elif "429" in str(e):
                    st.warning("âš ï¸ ØªÙ… ØªØ¬Ø§ÙˆØ² Ø­Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§ØªØŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹")

# ==================== ØªØ°ÙŠÙŠÙ„ ====================
st.divider()
col1, col2, col3 = st.columns(3)
with col1:
    st.caption(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„: {len([m for m in st.session_state.messages if m['role'] == 'user'])}")
with col2:
    st.caption(f"ğŸ§  Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {selected_model}")
with col3:
    st.caption("ğŸ’ Powered by GLM API")
