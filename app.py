import streamlit as st
import requests
import json

# --- 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ---
st.set_page_config(page_title="Multi-Model Debugger", page_icon="ğŸ› ï¸")

# ØªØ®ØµÙŠØµ CSS
st.markdown("""
<style>
    .stChatMessage { direction: rtl; text-align: right; }
    .stTextInput > div > div > input { direction: rtl; text-align: right; }
    .stSelectbox > div > div > div { direction: rtl; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ› ï¸ ÙØ­Øµ Ù…ÙˆØ¯ÙŠÙ„Ø§Øª (Cerebras + GLM)")

# --- 2. Ø§Ù„Ù…ÙØ§ØªÙŠØ­ (Keys) ---
# Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ù…ÙØªØ§Ø­ Cerebras
try:
    cerebras_key = st.secrets["CEREBRAS_API_KEY"]
except:
    cerebras_key = st.sidebar.text_input("Cerebras API Key:", type="password")

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ù…ÙØªØ§Ø­ GLM
try:
    glm_key = st.secrets["GLM_API_KEY"]
except:
    glm_key = st.sidebar.text_input("GLM (Zhipu) API Key:", type="password")

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ù‚Ø¨Ù„ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© (ØªØ­Ø°ÙŠØ± ÙÙ‚Ø·)
if not cerebras_key and not glm_key:
    st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙØªØ§Ø­ API ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„.")
    st.stop()

# --- 3. Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ---
with st.sidebar:
    st.header("Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„")
    
    # Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø­Ø¯Ø«Ø©
    models = [
        "llama-3.3-70b",   # Cerebras
        "llama3.1-8b",     # Cerebras
        "glm-4",           # GLM (ZhipuAI) - Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ‚Ø±
        "glm-4-plus",      # GLM (ZhipuAI) - Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø£Ù‚ÙˆÙ‰
        "qwen-3-32b",      # Cerebras
    ]
    
    selected_model = st.radio("Ø§Ø®ØªØ± Ù…ÙˆØ¯ÙŠÙ„ Ù„Ù„ØªØ¬Ø±Ø¨Ø©:", models)
    
    if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"):
        st.session_state.messages = []
        st.rerun()

# --- 4. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ© (ØªØ®ØªØ§Ø± Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ§Ù„Ù…ÙØªØ§Ø­ Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„) ---
def stream_chat_debug(messages, selected_model, c_key, g_key):
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
    if "glm" in selected_model.lower():
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª GLM (ZhipuAI)
        url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
        api_key = g_key
        if not api_key:
            yield "â›” **Ø®Ø·Ø£:** Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙØªØ§Ø­ GLM."
            return
    else:
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Cerebras Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        url = "https://api.cerebras.ai/v1/chat/completions"
        api_key = c_key
        if not api_key:
            yield "â›” **Ø®Ø·Ø£:** Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙØªØ§Ø­ Cerebras."
            return

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": selected_model,
        "messages": messages,
        "stream": True,
        "max_tokens": 1000
    }
    
    try:
        response = requests.post(url, headers=headers, json=data, stream=True)
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø®Ø·Ø£ Ù…Ù† Ø§Ù„Ø³ÙŠØ±ÙØ±
        if response.status_code != 200:
            error_details = response.text
            try:
                error_json = response.json()
                error_msg = error_json.get('error', {}).get('message', error_details)
                yield f"â›” **ÙØ´Ù„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„:** {selected_model}\n\n**Ø§Ù„Ø³Ø¨Ø¨:** {error_msg}"
            except:
                yield f"â›” **Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ:** Ø±Ù…Ø² Ø§Ù„Ø­Ø§Ù„Ø© {response.status_code}\n{error_details}"
            return

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨Ø« (Streaming)
        for line in response.iter_lines():
            if line:
                decoded = line.decode('utf-8').replace("data: ", "")
                if decoded.strip() == "[DONE]": break
                try:
                    chunk = json.loads(decoded)
                    # GLM Ùˆ Cerebras ÙŠØ´ØªØ±ÙƒØ§Ù† ÙÙŠ Ù†ÙØ³ Ù‡ÙŠÙƒÙ„ÙŠØ© Ø§Ù„Ø±Ø¯ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ (OpenAI Compatible)
                    content = chunk['choices'][0]['delta'].get('content', '')
                    if content: yield content
                except: continue
                
    except Exception as e:
        yield f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª: {e}"

# --- 5. Ø§Ù„ØªØ´ØºÙŠÙ„ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        response_holder = st.empty()
        full_text = ""
        
        # ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø®ØªØ§Ø± Ù„Ù„Ø¯Ø§Ù„Ø©
        stream_gen = stream_chat_debug(
            st.session_state.messages, 
            selected_model, 
            cerebras_key, 
            glm_key
        )
        
        for chunk in stream_gen:
            full_text += chunk
            response_holder.markdown(full_text + "â–Œ")
        
        response_holder.markdown(full_text)
        
        if "â›”" not in full_text:
            st.session_state.messages.append({"role": "assistant", "content": full_text})
